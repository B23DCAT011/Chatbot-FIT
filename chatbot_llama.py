# -*- coding: utf-8 -*-
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.llms import CTransformers
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
model_file = "C:/Users/Admin/Desktop/RAG_demo/models/vinallama-7b-chat_q5_0.gguf"
embedding_file = "C:/Users/Admin/Desktop/RAG_demo/models/all-MiniLM-L6-v2-f16.gguf"
vt_db_path = "vector_db/db_faiss"

# Prompt m·∫´u
template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, d·ªÖ hi·ªÉu, v√† ch·ªâ d√πng th√¥ng tin trong ph·∫ßn {context} ·ªü tr√™n. 
Kh√¥ng suy ƒëo√°n. Kh√¥ng lan man. N·∫øu c√¢u h·ªèi l√† y√™u c·∫ßu, h√£y ph·∫£n h·ªìi l·ªãch s·ª± nh∆∞ng d·ª©t kho√°t. N·∫øu ng∆∞·ªùi d√πng ch·ªâ c·∫ßn x√°c nh·∫≠n, h√£y tr·∫£ l·ªùi g·ªçn v√† t·ª± nhi√™n nh∆∞: "ƒê√£ r√µ!", "OK, t√¥i hi·ªÉu.", ho·∫∑c "T√¥i s·∫Ω th·ª±c hi·ªán ƒëi·ªÅu ƒë√≥.".
C√¢u h·ªèi: {question}
"""
# Bi·∫øn to√†n c·ª•c
llm_chain = None

# Ch·ªâ t·∫°o prompt template m·ªôt l·∫ßn
def create_prompt(tmp):
    return PromptTemplate(template=tmp, input_variables=["context", "question"])

# Ch·ªâ t·∫°o QA chain khi c·∫ßn
def init_llm_chain():
    global llm_chain
    print("üß† Loading model and vector DB...")
    db = read_from_db()
    llm = load_model(model_file)
    prompt = create_prompt(template)
    llm_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={'prompt': prompt},
    )
    print("‚úÖ LLM chain is ready.")

def load_model(model_file):
    return CTransformers(
        model=model_file,
        model_type="llama",
        max_new_tokens=256,
        temperature=0.01,
    )

def read_from_db():
    embedding = GPT4AllEmbeddings(model_file=embedding_file)
    db = FAISS.load_local(vt_db_path, embedding, allow_dangerous_deserialization=True)
    return db

def get_answer(question):
    init_llm_chain()
    result = llm_chain.invoke({"query": question})
    return result["result"]
